%
% File nonlinear.tex
%
\documentclass[11pt]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
%\usepackage{tikz}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage{multirow}
%\usepackage{hyperref}
%\usetikzlibrary{bayesnet}
\usepackage[mathscr]{euscript}
\usepackage{pgfplots}
\usepackage{relsize}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage{placeins}


\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

%\DeclareMathSizes{10}{10}{10}{10}
\everymath=\expandafter{\the\everymath\displaystyle}
\setlength\titlebox{2.8cm}    % Expanding the titlebox
%\def\layersep{2.5cm}
%\def\seqwd#1{$\strut$\small \emph{#1}}
%\def\seqwdg#1{\hbox to 0.6cm{\hss $\strut$\small \emph{#1} \hss}}
%\def\seqnd{\hbox to 0.7cm{}}
\newcommand{\redt}[1]{\textcolor{red}{#1}}
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\softmax}{softmax}
%\DeclareMathOperator*{\argmin}{arg\,min}

\def\argmax{\mathop{\mathrm{argmax}}}
\def\argmin{\mathop{\mathrm{argmin}}}
\def\softmax{\mathop{\mathrm{softmax}}}

\makeatletter
\def\@biblabel#1{}
\makeatother

\title{Better Chinese Word Segmentation using Dual Decomposition}

% \author{Mengqiu Wang\\
%  	    Computer Science Department\\
%  	    Stanford University\\
%  	    Stanford, CA 94305, USA\\
%  	    {\tt mengqiu@cs.stanford.edu}
%  	  \And
%  	Christopher D. Manning\\
% 	Computer Science Department\\Neural Nets
%    	Stanford University\\
%    	Stanford, CA 94305, USA\\
%    {\tt manning@cs.stanford.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

Cite \cite{Sun:2013:IPM}.

\end{abstract}

\section{Introduction}

\begin{itemize}
\item CWS is essential for many downstream applications and its performance has direct impacts
\item Two primary approaches: character-based and word-based
\item Character based includes work like:
\item Word based includes work like:
\item In both cases out-of-vocabulary recall is a problem, but char-based sequence models win
\item Solution has been proposed: mix models
\item Existing work (Sun paper?) trains many models and uses voting
\item We propose a simpler and more direct solution to model mixing: dual decomp to have two simultaneous mutually informative models
\end{itemize}

\section{Methodology}

\begin{itemize}
\item TODO: Work on this section together - explain dual decomp in this context in a simple way
\item proposal: if Rob can understand the methodology then your average non-dual-decomp expert will understand it
\end{itemize}

\section{Experiments}

\subsection{Accuracy}
\begin{itemize}
\item show that this model does better than existing work
\item biggest win is on R$_{\mathrm{oov}}$
\end{itemize}


\subsection{Efficiency}
\begin{itemize}
\item show that this model is faster / simpler / etc than existing work
\item important that methodology section is clear enough that other people could implement this
\end{itemize}



\begin{table}[h]
\centering
\begin{small}
\begin{tabular}{  l | c | c | c | c | c | c   }
 &  \multicolumn{1}{c}{Recall} &  \multicolumn{1}{c}{Precision}  & \multicolumn{1}{c}{F$_1$}   &  \multicolumn{1}{c}{OOV} &   \multicolumn{1}{c|}{R$_{\mathrm{oov}}$}    &  \multicolumn{1}{c}{R$_{\mathrm{iv}}$}   \\ 

\hline
CRF          & 0.978  &  0.969  &  0.973   & 0.035  & 0.723 & 0.987 \\
PCT      & 0.978 & 0.971 & 0.974 & 0.035 & 0.730 & 0.987 \\
DD     & \textbf{0.981} & \textbf{0.973} & \textbf{0.977} & 0.035 & \textbf{0.741} & \textbf{0.989} \\
\end{tabular} 
\caption{Results on CTB-6 dataset. }\label{tbl:ctb-results}
\end{small}
\end{table}


\addtocounter{footnote}{-1}

%\begin{table*}[ht]
\begin{table*}
\centering
%\begin{small}

\begin{tabular}{ l | l | c | c | c | c | c | c   }
\multicolumn{8}{c}{\large{SIGHAN 2005}} \\
%\cline{2-10}
%\hline
% &   \multicolumn{1}{|c}{Models} & \multicolumn{1}{c}{$\lambda$} & \multicolumn{1}{c|}{No. of experts} & {P} & {R} & {F$_1$}   & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c}{F$_1$}   \\
\hline
    \multicolumn{2}{c}{}  &  \multicolumn{1}{c}{Recall} &  \multicolumn{1}{c}{Precision}  & \multicolumn{1}{c}{F$_1$}   &  \multicolumn{1}{c}{OOV} &   \multicolumn{1}{c|}{R$_{\mathrm{oov}}$}    &  \multicolumn{1}{c}{R$_{\mathrm{iv}}$}   \\ 
\hline
\multirow{4}{*}{AS} & SIGHAN winner      & 0.952 &  \textbf{0.951} & 0.952 &  0.043  &  0.696 &  0.963 \\
& CRF-Char                                                        & 0.952 &   0.936 & 0.944 & 0.043 &  0.589 & 0.969 \\
 & Perceptron-Word                                       & 0.958 & 0.950 & 0.954 & 0.043 & 0.695 & 0.970 \\ 
& Dual Decomp                                       & 0.959 & 0.949 & \textbf{0.954} & 0.043 & {0.677} & \textbf{0.972} \\
\hline

\multirow{4}{*}{PKU} &  SIGHAN winner       & \textbf{0.953}  &      0.946 &  0.950 & 0.058  &  0.636 &  \textbf{0.972} \\
& CRF-Char        & 0.946 &	0.953 & 0.949 &  0.058 &   0.778 &  0.956 \\
& Perceptron-Word  & 0.941 & 0.955	& 0.948 &  0.058 &  0.767 &  0.952 \\
& Dual Decomp  & 0.948 & \textbf{0.957} & \textbf{0.953} &  0.058 &  \textbf{0.787} &0.958 \\
\hline

\multirow{4}{*}{CITYU}  & SIGHAN winner       &  0.941 &  \textbf{0.946} & 0.943 & 0.074 & 0.698 & 0.961 \\
& CRF-Char       & 0.947 & 0.940 &  0.943 &  0.074 & \textbf{0.761} & 0.962 \\
& Perceptron-Word & 0.943 & 0.940 & 0.942 & 0.074 & 0.717  & 0.961  \\
& Dual Decomp & \textbf{0.950} & 0.944 & \textbf{0.947} & 0.074 & {0.753} & \textbf{0.965}  \\

\hline
\multirow{4}{*}{MSR} &  SIGHAN winner      & 0.962  & 0.966 &  0.964 &  0.026 &   0.717 &   0.968 \\
& CRF-Char        &  0.964 &   0.966 &  0.965 & 0.026 & 0.713 & 0.971 \\
& Perceptron-Word &  0.970 &  0.972 &  0.971 & 0.026 & 0.746 & 0.976 \\
& Dual Decomp & \textbf{0.973} &  \textbf{0.974} &  \textbf{0.974} & 0.026 & \textbf{0.760} & \textbf{0.979} \\
\hline
\multicolumn{8}{c}{\large{SIGHAN 2003}} \\
\hline

\multirow{4}{*}{AS}  &  SIGHAN winner     & 0.966     & 0.956 &  0.961 & 0.022 & 0.364 &  \textbf{0.980} \\
& CRF-Char    & 0.969 & 0.969 & 0.969 & 0.022 & 0.748 &  0.974 \\
& Perceptron-Word &  0.967 & 0.967 & 0.967 & 0.022 & 0.729  & 0.972 \\
& Dual Decomp & \textbf{0.970} & \textbf{0.971} & \textbf{0.971} & 0.022 & \textbf{0.775}  & 0.975 \\
\hline
\multirow{4}{*}{PKU}  &  SIGHAN winner        &   \textbf{0.962} &   0.940&  0.951 & 0.069 & 0.724 & \textbf{0.979} \\
& CRF-Char     &  0.954 & 0.952 & 0.953 & 0.069 & 0.803	 & 0.965 \\
& Perceptron-Word &  0.949 & 0.952 & 0.950 & 0.069 & 0.790  & 0.960 \\
& Dual Decomp &  0.954 & \textbf{0.954} & \textbf{0.954}	 & 0.069 & \textbf{0.806} & 0.965 \\
\hline
\multirow{4}{*}{CITYU}  &  SIGHAN winner   &  0.947   & 0.934  & 0.940 & 0.071  &  0.625  &  \textbf{0.972} \\
& CRF-Char       &  0.944 & 0.939 & 0.941 & 0.071  & 0.741   & 0.959 \\
& Perceptron-Word &  0.944 & 0.945 & 0.945 & 0.071  & 0.730  &  0.960 \\
& Dual Decomp &  \textbf{0.950} & \textbf{0.949} & \textbf{0.949} & 0.071 & \textbf{0.754} &  0.965 \\
\hline
\multirow{4}{*}{CTB}  &  SIGHAN winner  &  \textbf{0.886}  & 0.875 &  \textbf{0.881} & 0.181 & 0.644  & \textbf{0.927} \\
& CRF-Char       & 0.869 & 0.865 & 0.867 & 0.181 & 0.680  & 0.910 \\
& Perceptron-Word & 0.865 & 0.871 & 0.868 & 0.181 & 0.660 & 0.910 \\
& Dual Decomp  & 0.876 & \textbf{0.878} & 0.877\footnotemark & 0.181 & \textbf{0.692} & 0.917  \\


\end{tabular} 
\caption{Results on SIGHAN 2005 and 2003 datasets. }\label{tbl:results}
%\end{small}
\end{table*}

\footnotetext{This result would place 2nd in the SIGHAN competition.}

\section{Conclusion}

\bibliographystyle{naaclhlt2013}
\bibliography{cws}

\end{document}
