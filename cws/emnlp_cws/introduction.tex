\section{Introduction}

%First paragraph talks about why Chinese Word Segmentation is important.
Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by [cite, cite, cite - Pichuan, what else?], the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, [and...].

%Second paragraph talks about the challenges, specifically ambiguity and unknown words.
State-of-the-art performance in CWS is high, with F-scores in the upper 90s. %say this in a nicer way?
Still, challenges remain. Unknown words, also known as out-of-vocabulary (\textsc{oov}) words, lead to difficulties for word- or dictionary-based approaches.
Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (``talent") and 才 / 能 (``just able"). % from Gao acl 2003

%Third paragraph: 
There are two primary classes of models: character-based \cite{Xue:2003:IJCLCLP,Tseng:2005:SIGHAN,Zhang:2006:HLT-NAACL,Wang:2010:COLING} and word-based \cite{Andrew:2006:EMNLP,Zhang:2007:ACL}, with corresponding advantages and disadvantages. Character-based segmenters better model the internal compositional structure of words and are therefore more effective at detecting out-of-vocabulary words; however, they suffer from relative inconsistency. % problematic - our experiments show this isn't really true, the CRF is more consistent 6/8 times
Word-based segmenters are better at reproducing the words of the training lexicon and have a higher consistency, but are not as good at detecting \textsc{oov} words. Word-based models have the additional advantage that they tend to capture information from significantly larger contextual spans.
% too detailed?
%Due to the Markov assumption of the linear-chain CRFs that are commonly used in character-based segmentation the models typically only have information about the immediately neighboring characters, while word-based methods tend to capture information from significantly larger contextual spans.

%An additional advantage of word-segmenter is that they can explore larger context than character-based models (this is a little hard to explain without getting into the details of how these models work, maybe leave this piece till later; the basic idea is that character-based models -- i.e., linear-chain CRFs --- have very limited context due to Markov assumption, it typically only have direct information about the immediate neighboring characters, although information from longer-range context does flow because of the sentence-level normalization, its effect is much less direct. Word-based models in contrary directly captures the formation of entire words, say the previous word token has 3 characters, and the current word being considered has 3 characters, then a word-level bigram feature would capture context in neighboring 6 characters. 

In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition. This method has strong optimality guarantees and works very well empirically. It is easy to implement and does not require retraining of existing character- and word-based segmenters.
Experimental results on standard SIGHAN 2003 and 2005 bake-off evaluations show that our model outperforms the character and word baselines by a significant margin on all 8 datasets. % wait, aren't we showing results for only 7?
In particular, it improves \textsc{oov} recall rates and consistency, % this is now validated
and gives the best reported results to date on 6 out of 7 datasets. 