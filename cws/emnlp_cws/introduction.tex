\section{Introduction}

%First paragraph talks about why Chinese Word Segmentation is important.
Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by [cite, cite, cite - Pichuan, what else?], the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, [and...].

%Second paragraph talks about the challenges, specifically ambiguity and unknown words.
State-of-the-art performance in CWS is high, with F-scores in the upper 90s. %say this in a nicer way?
Still, challenges remain. Unknown words, also known as out-of-vocabulary (\textsc{oov}) words, lead to difficulties for word- or dictionary-based approaches.
Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (``talent") and 才 / 能 (``just able") \cite{Gao:2003:ACL}.

%Third paragraph: 
There are two primary classes of models: character-based \cite{Xue:2003:IJCLCLP,Tseng:2005:SIGHAN,Zhang:2006:HLT-NAACL,Wang:2010:COLING} and word-based \cite{Andrew:2006:EMNLP,Zhang:2007:ACL}, with corresponding advantages and disadvantages. \cite{Sun:2010:COLING} details their theoretical distinctions: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new out-of-vocabulary words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive.
% too detailed?
%Due to the Markov assumption of the linear-chain CRFs that are commonly used in character-based segmentation the models typically only have information about the immediately neighboring characters, while word-based methods tend to capture information from significantly larger contextual spans.
%An additional advantage of word-segmenter is that they can explore larger context than character-based models (this is a little hard to explain without getting into the details of how these models work, maybe leave this piece till later; the basic idea is that character-based models -- i.e., linear-chain CRFs --- have very limited context due to Markov assumption, it typically only have direct information about the immediate neighboring characters, although information from longer-range context does flow because of the sentence-level normalization, its effect is much less direct. Word-based models in contrary directly captures the formation of entire words, say the previous word token has 3 characters, and the current word being considered has 3 characters, then a word-level bigram feature would capture context in neighboring 6 characters. 

In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition. This method has strong optimality guarantees and works very well empirically. It is easy to implement and does not require retraining of existing character- and word-based segmenters.
Experimental results on standard SIGHAN 2003 and 2005 bake-off evaluations show that our model outperforms the character and word baselines by a significant margin.
In particular, it improves \textsc{oov} recall rates and segmentation consistency, % this is now validated
and gives the best reported results to date on 6 out of 7 datasets.