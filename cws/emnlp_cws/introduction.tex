\section{Introduction}

First paragraph talks about why Chinese Word Segmentation is important.

Second paragraph talks about the challenges, specifically ambiguity and unknown words.

Third paragraph: There are two classes of models: character-based \cite{Xue:2003:IJCLCLP,Tseng:2005:SIGHAN,Zhang:2006:HLT-NAACL,Wang:2010:COLING} and word-based \cite{Andrew:2006:EMNLP,Zhang:2007:ACL}, each has their respective advantage and disadvantage: char-based segmenters models the internal structure of word composition better and therefore are more effective at detecting out-of-vocabulary words, however they suffer from inconsistency issues. Word-segmenters are better at memorizing the training word type lexicon, and have better consistency, but not as good at detecting OOV words. An additional advantage of word-segmenter is that they can explore larger context than character-based models (this is a little hard to explain without getting into the details of how these models work, maybe leave this piece till later; the basic idea is that character-based models -- i.e., linear-chain CRFs --- have very limited context due to Markov assumption, it typically only have direct information about the immediate neighboring characters, although information from longer-range context does flow because of the sentence-level normalization, its effect is much less direct. Word-based models in contrary directly captures the formation of entire words, say the previous word token has 3 characters, and the current word being considered has 3 characters, then a word-level bigram feature would capture context in neighboring 6 characters. 

Fourth paragraph (maybe consider moving some of this into a related work section and put it at the end): various mixing approaches have been proposed to combine the merits of these two classes of models \cite{Wang:2006:SIGHAN,Lin:2009:CICLing,Sun:2009:HLT-NAACL,Sun:2010:COLING,Wang:2010:COLING}. 

\cite{Sun:2009:HLT-NAACL} is the most similar in that they also try to directly combine two segmenters -- one char-based and one word-based. But they do it through segmenter bagging, which would require training up 50 or more individual segmenters and poll their results at test time, significantly increases training and testing effort, not practical.
\cite{Sun:2009:HLT-NAACL} presented a latent-variable model. very complicated, hard to train. our method is much simpler and works better than their method.

\cite{Lin:2009:CICLing} and \cite{Wang:2006:SIGHAN} both try to incorporate a character-based local classifier model (MaxEnt) with a language model that captures more word-level context.
In order to bring in bigram language model,  \shortcite{Lin:2009:CICLing} gave a heuristic decoding method that involves various conditioning and back-off; \shortcite{Wang:2006:SIGHAN} gave a modified Viterbi algorithm actually has complexity of $O(T^3)$. 

In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenter based on dual decomposition. This method has strong optimality guarantees and works very well empirically. It's easy to implement and does not require retraining of existing character and word segmenters.
Experimental results on standard SIGHAN 2003 and 2005 bake-off evaluations show that our model consistently outperform the word and character baselines by significant margin on all 8 datasets. In particular, it improves OOV recall rates and consistency (yet to be validated pending consistency experiment results). It gives the best reported results to date on 6 out of 8 datasets. 
