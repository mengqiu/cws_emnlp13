\section{Discussion}

%In this section we talked about detailed consistency analysis and maybe give some examples.

\subsection{Dual decomposition convergence}
Plot the histogram of the number of iterations it takes to converge, and percentage of optimality. 

\subsection{Consistency}

\cite{Chang:2008:ACL} have shown that increased segmentation consistency is correlated with better machine translation performance. Following their method for calculating the conditional entropy of a segmentation system, we see in Table [insert table] that our dual decomposition method achieves the most consistent results on 6 out of 7 datasets.

Consistency Results, Sighan 2003:

as crf
0.037814800237
as pct
0.038997075095
as dd
0.0372484903836
cityu crf
0.0923886605473
cityu pct
0.097496193366
cityu dd
0.0843064189868
ctb crf
0.170532711404
ctb pct
0.181993909505
ctb dd
0.161722439818
pku crf
0.0429803512049
pku pct
0.0552572297562
pku dd
0.0460509973844


Consistency Results, Sighan 2005:

as crf
0.0635314146511
as pct
0.0603263902622
as dd
0.055097582491
cityu crf
0.0649537861096
cityu pct
0.0730776631941
cityu dd
0.0618197636595
msr crf
0.073782773985
msr pct
0.0628241148726
msr dd
0.054883418641
pku crf
0.0888607888366
pku pct
0.0996634383663
pku dd
0.086635535204

\subsection{Oracle}

Following \cite{Sun:2010:COLING}, we run an oracle experiment to estimate the upper bound of improvement possible via system combination to further contextualize our results. 

To do so, we combine our two baselines with the gold-standard segmentation. Each character in the test set is labeled with three \emph{B} or \emph{I} tags, \emph{B} when it begins a word and \emph{I} when it is word-medial or word-final, according to our two baselines and the gold standard. We then create oracle labels by majority vote: if the baselines agree, their label is used; if they disagree, the gold label is used.

The results of this oracle experiment, shown in Table [insert table], show that [what do they show? DD approaches the upper bound possible?]

% this is here to know what each column means in the output below
%=== TOTAL INSERTIONS:	75
%=== TOTAL DELETIONS:	84
%=== TOTAL SUBSTITUTIONS:	783
%=== TOTAL NCHANGE:	942
%=== TOTAL TRUE WORD COUNT:	11989
%=== TOTAL TEST WORD COUNT:	11980
%=== TOTAL TRUE WORDS RECALL:	0.928
%=== TOTAL TEST WORDS PRECISION:	0.928
%=== F MEASURE:	0.928
%=== OOV Rate:	0.022
%=== OOV Recall Rate:	0.810
%=== IV Recall Rate:	0.930
Scoring Oracle Output, Sighan 2003
for as
	sighan2003/as.oracle	75	84	783	942	11989	11980	0.928	0.928	0.928	0.022	0.810	0.930
for cityu
	sighan2003/cityu.oracle	402	333	2364	3099	35087	35156	0.923	0.921	0.922	0.071	0.824	0.931
for ctb
	sighan2003/ctb.oracle	920	1005	4490	6415	39921	39836	0.862	0.864	0.863	0.181	0.773	0.882
for pku
	sighan2003/pku.oracle	186	187	1593	1966	17194	17193	0.896	0.897	0.897	0.069	0.834	0.901

Scoring Oracle Output, Sighan 2005
for as
	sighan2005/as.oracle	1855	726	9980	12561	122610	123739	0.913	0.904	0.908	0.043	0.722	0.921
for cityu
	sighan2005/cityu.oracle	577	371	3038	3986	40936	41142	0.917	0.912	0.914	0.074	0.817	0.925
for msr // currently broken
	sighan2005/msr.oracle	81	107	1066	1254	10987	10961	0.893	0.895	0.894	0.025	0.584	0.901
for pku
	sighan2005/pku.oracle	800	1661	9646	12107	104372	103511	0.892	0.899	0.895	0.058	0.815	0.896


\subsection{Error analysis}
% this makes sense, i will do this tomorrow
Maybe look at cases where the model choose the worse of the two instead of the better of the two. See if there are patterns or insights we can draw. Not very important for a short paper.

some errors where DD picks the wrong model:
gold: 有 亭台樓閣 流水
CRF: 亭台 樓閣  <-- DD
PCT: 亭台樓閣

gold: 大規模 海陸空 聯合 軍演   
CRF: 大規模 海 陸空 聯合 軍演   <-- DD
PCT: 大規模 海陸空 聯合 軍演   


gold: 在 李鄭屋邨 附近
CRF: 在 李鄭屋邨 附近
PCT: 在 李鄭屋 邨 附近   <-- DD

gold: 麥 父 笑說
CRF:  麥 父 笑說
PCT: 麥父 笑說    <-- DD

gold: 天氣 熱 時 冷氣 大 開     
CRF: 天氣 熱 時 冷氣 大 開     
PCT: 天氣 熱時 冷氣 大 開     <-- DD
