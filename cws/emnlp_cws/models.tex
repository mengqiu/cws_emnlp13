\section{Models for CWS}

In this section, we describe the character-based and word-based models we use as baselines, and review existing approaches to combine these models.

\subsection{Character-based Models}
In the most commonly used contemporary approach to character-based segmentation, first proposed by \cite{Xue:2003:IJCLCLP}, CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRFs) are often used for this purpose (cite, cite, cite). In a CRF segmentation model, the probability of a label sequence is given by this equation:

%[add eq here]
% add defs of all the variables

Common linguistic features include character n-grams and morphological suffix/prefix features. Since these features capture information about the compositional properties of characters, they are likely to generalize well to unknown words.


\subsection{Word-based Models}

Initially word-based segmentation approaches employed simple heuristics like dictionary-lookup maximum matching \cite{Chen:1992:ACL}, contemporary approaches use machine learning techniques to solve an argmax problem of the form:

% add eq

The most successful such system reported to date is \cite{Zhang:2007:ACL}'s Perceptron-based model, which uses a search-based discriminative decoder to solve the ... %MQ add here?


%See \cite{Sun:2010:COLING} for a good description. We used the perceptron based model proposed by \cite{Zhang:2007:ACL}, give the equation of the perceptron, and describe the features. Point out how some of these features basically directly capture the training lexicon.
%
%\subsection{Relative Strength and Weakness}
%Char-based CRF models are better at generalizing over to OOV words (thus higher OOV recall).  (NOTE: 6 out of 8 datasets on test, CRF has better OOV recall, but we should defer to talk about this in the results section). But as a tradeoff, it generates more inconsistent segmentation, and creates lots of spurious words.
% The advantage of Word-based segmenters are that they are more consistent with the training lexicon, words seen in training lexicon are not likely to be segmented into many new ways; disadvantage is that it doesn't generalize well to OOV. Also exact inference is difficult, need to resolve to approximate decoding algorithms such as beam-searching.
% 

\subsection{Mixing Models} Since the two types of models described above have different strengths and make different kinds of errors, various mixing approaches have been proposed to combine them \cite{Wang:2006:SIGHAN,Lin:2009:CICLing,Sun:2009:HLT-NAACL,Sun:2010:COLING,Wang:2010:COLING}. 


\cite{Lin:2009:CICLing} and \cite{Wang:2006:SIGHAN} both incorporate a character-based Maxent local classifier model with a language model that captures more word-level context.
In order to bring in a bigram language model,  \shortcite{Lin:2009:CICLing} gave a heuristic decoding method that involves various forms of conditioning and back-off; \shortcite{Wang:2006:SIGHAN} gave a modified Viterbi algorithm with a complexity of $O(T^3)$. 

\cite{Sun:2009:HLT-NAACL} presented a latent-variable model which obtains good performance, but it is very complicated to implement and difficult to train. 
\cite{Sun:2009:HLT-NAACL} use a method most similar to this work in that they also directly combine two segmenters -- one char-based and one word-based. However, they do it through segmenter bagging, which requires training 50 or more individual segmenters and polling their results at test time. 

These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation.