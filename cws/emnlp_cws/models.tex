\section{Models for CWS}

In this section, we describe the character-based and word-based models we use as baselines, and review existing approaches to combine these models.


\subsection{Character-based Models}
In the most commonly used contemporary approach to character-based segmentation, first proposed by \cite{Xue:2003:IJCLCLP}, CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) \cite{Lafferty:2001:ICML} are widely adopted for this task, and give state-of-the-art results \cite{Tseng:2005:SIGHAN}. In a first-order linear-chain CRF model, the conditional probability of a label sequence $\mathbf{y}$ given a word sequence $\mathbf{x}$ is defined as:
\begin{align*}\vspace{-10em}
P(\mathbf{y} | \mathbf{x}) = \frac{1}{Z}{ \sum\limits_{t=1}^{|\mathbf{y}|}{ \exp\left( \theta \cdot {f}(x, y_t, y_{t+1}) \right) }}
\vspace{-10em}
\end{align*}
\noindent $f(x, y_{t}, y_{t-1})$ are feature functions that typically include surrounding n-gram and morphological suffix/prefix features. These types of features capture the compositional properties of characters and are likely to generalize well to unknown words.  
However, the Markov assumption in CRF limits the context of such features; it is difficult to capture long-range word features in this model. 
%, which is important for admitting efficient inference, restricts the CRF to very local context features. A first-order CRF can only reason about character boundary decisions of current and neighboring positions. 
%It is therefore difficult to directly model words of greater length. Although higher-order CRF could potentially alleviate this problem, and has been applied to CWS \cite{Zhao:2007:ICNC}, it comes at the cost of exponentially higher model complexity. 

\subsection{Word-based Models}
Word-based models search through lists of word candidates using scoring functions that directly assign scores to each. 
Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching \cite{Chen:1992:ACL}. 
More recently, \newcite{Zhang:2007:ACL} reported success using a linear model trained with the average perceptron algorithm \cite{Collins:2002:EMNLP}.
 Formally, given input $\mathbf{x}$, their model seeks a segmentation $F(\mathbf{x})$ such that:
\begin{align*}
F(\mathbf{y}|\mathbf{x}) =  \argmax{ \left( \alpha \cdot \phi(\mathbf{\mathbf{y}}) \right) }
\end{align*}
\noindent Searching through the entire $\mathrm{GEN}(\mathbf{x})$ space is intractable even with a local model, so a beam-search algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position.

\begin{algorithm}[!ht]
\begin{footnotesize}
\caption{DD inference algorithm, modified Viterbi and beam-search.}
\begin{algorithmic}
\STATE $\forall i \in \{1\; \mathrm{to} \; |\mathbf{x}|\} \colon \; \forall k \in \{0,1\} \colon u_{i}(k) = 0$
\FOR{$t \leftarrow 1$  \textbf{to} $T$}
\STATE   $\mathbf{y^{c*}} = \argmaxb \; P(\mathbf{y^\textit{c}} | \mathbf{x}) +  \sum\limits_{i\in |\mathbf{x}|}{u_{i}(y_i^\textit{c})}$ 
\STATE   $\mathbf{y^{w*}} = \argmax \; F(\mathbf{y^\textit{w}} | \mathbf{x}) -  \sum\limits_{i\in |\mathbf{x}|}{ u_{i}(y_j^\textit{w})}$
\IF{ $\mathbf{y^{c*}} =  \mathbf{y^{w*}}$ }
\RETURN $\left(\mathbf{y^{\textit{c}*}},\mathbf{y^{\textit{w}*}}\right)$
\ENDIF
\FORALL{$i \in \{1\; \mathrm{to} \; |\mathbf{x}|\} $}
\STATE  $\forall k \in \{0,1\}: u_i(k)=u_i(k) + \alpha_t (2k-1)(y_i^{w*}-y_i^{c*})$ 
\ENDFOR
\ENDFOR
\RETURN $\left(\mathbf{y^{c*}},\mathbf{y^{w*}}\right)$
 \\\hrulefill
\STATE Viterbi: 
\STATE $V_1(1) = 1, V_1(0) = 0$
\FOR{$i = 2 \; \mathrm{to} \; |\mathbf{x}|$}
\STATE $\forall k \in \{0,1\}: V_i(k) = \argmaxk P_i(k | k') V_{i-1}{k'} + u_{i}(k)$ 
\ENDFOR
 \\\hrulefill
\STATE Beam-Search:
\FOR{$i = 1 \; \mathrm{to} \; |\mathbf{x}|$}
\FOR{ item $v = \{w_0,\cdots,w_j\}$ in $\mathrm{beam}(i)$}
\STATE append $x_i$ to $w_j$, $\mathrm{score}(v) \stackrel{+}{=}   u_{i}(0)$
\STATE $v = \{w_0,\cdots,w_j, x_i\}$, $\mathrm{score}(v) \stackrel{+}{=} u_{i}(1)$
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{algo:DD}
\end{footnotesize}
\end{algorithm}


\begin{table*}
\centering
\begin{small}
\begin{tabular}{ r | c | c | c | c | c | c | c | c | c | c  }
%\multicolumn{13}{c}{\large{SIGHAN 2005}} \\
\multicolumn{1}{c|}{} & \multicolumn{5}{c|}{AS} &  \multicolumn{5}{c}{PU} \\
\hline
\multicolumn{1}{c}{}   & \multicolumn{1}{|c}{R} &  \multicolumn{1}{c}{P}     &  \multicolumn{1}{c}{F$_1$}   &    \multicolumn{1}{c}{R$_{\mathrm{oov}}$}   &   \multicolumn{1}{c}{C$_{\mathrm{onst}}$}  & \multicolumn{1}{|c}{R}  &  \multicolumn{1}{c}{P}   &  \multicolumn{1}{c}{F$_1$}   &   \multicolumn{1}{c}{R$_{\mathrm{oov}}$}  &   \multicolumn{1}{c}{C$_{\mathrm{onst}}$} \\ 
Char-based CRF    &  95.2 &   93.6  & 94.4  &  58.9 & 0.064                                               &  94.6             &    95.3 & 94.9  &   77.8   & 0.089    \\
Word-based Perceptron &  95.8 & \textbf{95.0} & \textbf{95.4}  & \textbf{69.5} & 0.060  &  94.1             & 95.5  & 94.8  &  76.7 & 0.099     \\ 
Dual-decomp      & \textbf{95.9} & 94.9   & \textbf{95.4}  & {67.7} & \textbf{0.055}            & \textbf{94.8} & \textbf{95.7} & \textbf{95.3}  &  \textbf{78.7} & \textbf{0.086}   \\
%\cline{2-9}
\multicolumn{1}{c|}{} &  \multicolumn{5}{c|}{CU} &  \multicolumn{5}{c}{MSR}  \\
\hline
\multicolumn{1}{c}{}   & \multicolumn{1}{|c}{R} &  \multicolumn{1}{c}{P}     &  \multicolumn{1}{c}{F$_1$}   &    \multicolumn{1}{c}{R$_{\mathrm{oov}}$}   &   \multicolumn{1}{c}{C$_{\mathrm{onst}}$}  & \multicolumn{1}{|c}{R}  &  \multicolumn{1}{c}{P}   &  \multicolumn{1}{c}{F$_1$}   &   \multicolumn{1}{c}{R$_{\mathrm{oov}}$}  &   \multicolumn{1}{c}{C$_{\mathrm{onst}}$} \\ 
Char-based CRF                 & 94.7 & 94.0 & 94.3  & \textbf{76.1}                            & 0.065             & 96.4 &              96.6 &             96.5  & 71.3 & 0.074 \\
Word-based Perceptron    & 94.3 & 94.0 & 94.2  & 71.7                                         & 0.073              &  97.0 &              97.2                & 97.1  & 74.6 & 0.063  \\
Dual-decomp                   & \textbf{95.0} & \textbf{94.4}  & \textbf{94.7}  & {75.3} & \textbf{0.062} & \textbf{97.3} &  \textbf{97.4} &  \textbf{97.4}  & \textbf{76.0} & \textbf{0.055}  \\
\end{tabular} 
\caption{Results on SIGHAN 2005 datasets. {R$_{\mathrm{oov}}$}  denotes \textsc{oov} recall, and C$_{\mathrm{onst}}$ denotes segmentation consistency. Best number in each column is highlighted in bold. The naming of the datasets are: \emph{AS} --- Academia Sinica, \emph{PU} --- Peking University, \emph{CU} --- City Univeristy, \emph{MSR} ---  Microsoft Research.
%$\dagger$ and $\ddagger$ denote statistical significance ($p<0.01$) against CRF and perceptron baselines, respectively. 
}\label{tbl:results}
\end{small}
\end{table*}

\subsection{Combining Models with Dual Decomposition} 
Various mixing approaches have been proposed to combine the above two approaches \cite{Wang:2006:SIGHAN,Lin:2009:CICLing,Sun:2009:HLT-NAACL,Sun:2010:COLING,Wang:2010:COLING}. 
These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation.

Dual decomposition (DD) \cite{Rush:2010:EMNLP} offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to \cite{Sun:2009:HLT-NAACL}) or decoding efficiency (in contrast to bagging in \cite{Wang:2006:SIGHAN,Sun:2010:COLING}). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing \cite{Koo:2010:EMNLP}), bilingual sequence tagging \cite{Wang:2013:ACL} and word alignment \cite{Denero:2011:ACL}.  

The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely to agree on.
Formally, the objective of DD is:
\begin{align*}
   \max_{\mathbf{y^\textit{c}}, \mathbf{y^\textit{w}}}  \; P(\mathbf{y^\textit{c}} | \mathbf{x}) + F(\mathbf{y^\textit{w}} | \mathbf{x}) \;  \ni \mathbf{y^\textit{c}} = \mathbf{y^\textit{w}}
\end{align*}
\noindent where $\mathbf{y^\textit{c}}$ is the output of character-based CRF and $\mathbf{y^\textit{w}}$ is the output of word-based perceptron.

The DD algorithm is an iterative procedure: in each iteration, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other. This penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other. We give an updated Viterbi decoding algorithm for CRF and a modified beam-search algorithm for perceptron, as well as pseudo-code for DD algorithm in Algo.~\ref{algo:DD}\footnote{Due to space limitations, we defer to the tutorial of \newcite{Rush:2012:JAIR} for a full introduction of DD.}.


